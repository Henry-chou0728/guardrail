import os
import asyncio
from dotenv import load_dotenv
from nemoguardrails import LLMRails, RailsConfig
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma

# 1. è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# --- è¨­å®šè³‡æ–™åº« ---
embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")
DB_PATH = "./chroma_db"

vector_store = None
if os.path.exists(DB_PATH):
    try:
        vector_store = Chroma(
            persist_directory=DB_PATH,
            embedding_function=embedding_model
        )
        print(f"âœ… å·²é€£æ¥è‡³å‘é‡è³‡æ–™åº«: {DB_PATH}")
    except Exception as e:
        print(f"âŒ è³‡æ–™åº«é€£æ¥å¤±æ•—: {e}")
else:
    print("âš ï¸  è­¦å‘Šï¼šæ‰¾ä¸åˆ° chroma_db è³‡æ–™å¤¾ï¼")

# --- Action 1: æª¢ç´¢è³‡æ–™ (Retrieve) ---
async def retrieve_knowledge(query: str):
    print(f"\n[RAG Action] æ­£åœ¨æœå°‹: {query}")
    
    if not vector_store:
        return "è³‡æ–™åº«å°šæœªé€£æ¥ã€‚"

    try:
        results_with_score = vector_store.similarity_search_with_score(query, k=3)
        valid_results = []
        for doc, score in results_with_score:
            print(f"   - ç‰‡æ®µåˆ†æ•¸ (Distance): {score:.4f}") 
            if score < 1.8: 
                valid_results.append(doc.page_content)
        
        if not valid_results:
            return "æ²’æœ‰æ‰¾åˆ°ç›¸é—œçš„é‡‘èæ•¸æ“šã€‚"

        context_text = "\n\n".join(valid_results)
        print("   -> âœ… å·²æ‰¾åˆ°é«˜ç›¸é—œæ€§è³‡è¨Šã€‚")
        return context_text
        
    except Exception as e:
        print(f"æª¢ç´¢ç™¼ç”ŸéŒ¯èª¤: {e}")
        return "æª¢ç´¢éŒ¯èª¤"

async def generate_answer(context: str, question: str):
    print("   -> ğŸ¤– æ­£åœ¨ç”Ÿæˆå›ç­” (LLM)...")
    
    # åˆå§‹åŒ– LLM
    llm = ChatOpenAI(temperature=0.5, model="gpt-4o") # ç¨å¾®èª¿é«˜æº«åº¦è®“å°è©±è‡ªç„¶é»
    
    # ä¿®æ”¹ Promptï¼šå…è¨±åœ¨æ²’æœ‰ Context æ™‚ä½¿ç”¨é€šç”¨çŸ¥è­˜
    prompt = f"""
    ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„é‡‘è AI åŠ©ç†ã€‚è«‹å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚
    
    é‚è¼¯åˆ¤æ–·ï¼š
    1. è«‹å…ˆé–±è®€ä¸‹æ–¹çš„ã€Œå·²çŸ¥è³‡è¨Š (Context)ã€ã€‚
    2. å¦‚æœã€Œå·²çŸ¥è³‡è¨Šã€åŒ…å«å•é¡Œçš„ç­”æ¡ˆï¼Œè«‹**å„ªå…ˆ**æ ¹æ“šè³‡è¨Šå›ç­”ã€‚
    3. å¦‚æœã€Œå·²çŸ¥è³‡è¨Šã€èˆ‡å•é¡Œç„¡é—œï¼Œæˆ–è€…å•é¡Œåªæ˜¯æ‰“æ‹›å‘¼ï¼ˆå¦‚ Hi, ä½ å¥½ï¼‰æˆ–é€šç”¨æ¦‚å¿µï¼Œ**è«‹ä½¿ç”¨ä½ è‡ªå·±çš„çŸ¥è­˜å›ç­”**ï¼Œä¸è¦æ­»æ¿åœ°æ‹’çµ•ã€‚
    
    é™åˆ¶ï¼š
    - **å¿…é ˆ** ä½¿ç”¨ç¹é«”ä¸­æ–‡ (Traditional Chinese) å›ç­”ã€‚
    
    å·²çŸ¥è³‡è¨Š (Context):
    {context}
    
    ä½¿ç”¨è€…å•é¡Œ (Question): {question}
    
    å›ç­”:
    """
    
    response = await llm.ainvoke(prompt)
    return response.content

# --- ä¸»ç¨‹å¼ ---
async def main():
    config = RailsConfig.from_path("./config")
    rails = LLMRails(config)

    # â­ï¸ é—œéµï¼šè¨»å†Šå…©å€‹ Action
    rails.register_action(retrieve_knowledge, name="retrieve_knowledge")
    rails.register_action(generate_answer, name="generate_answer")

    print("\nğŸš€ --- é‡‘èæŒ‡å¼• RAG ç³»çµ±å•Ÿå‹• ---")
    
    while True:
        try:
            user_input = input("\nUser: ")
            if user_input.lower() in ["exit", "quit"]:
                print("Bye!")
                break
            
            response = await rails.generate_async(prompt=user_input)
            
            if hasattr(response, "response"):
                print(f"Bot: {response.response}")
            else:
                print(f"Bot: {response}")

        except Exception as e:
            print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")

if __name__ == "__main__":
    asyncio.run(main())